{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f55e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import ddg_images\n",
    "import requests\n",
    "import os\n",
    "\n",
    "#https://pypi.org/project/duckduckgo-search/#2-ddg_images---image-search-by-duckduckgocom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd6efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetDownloader:\n",
    "    def __init__(self, dataset_name, label_data, allowed_extensions):\n",
    "        \"\"\"\n",
    "            # label_data - list of pairs of (label, search word for label)\n",
    "        \"\"\"\n",
    "        self.dataset_name = dataset_name\n",
    "        self.label_data = label_data\n",
    "        self.allowed_extensions = allowed_extensions\n",
    "        \n",
    "        # prepare dataset folder\n",
    "        self.create_dir_if_not_exists(self.dataset_name)\n",
    "        \n",
    "    def download_images(self, search_limit_per_class=1):\n",
    "        \n",
    "        for label_name, search_keyword in self.label_data:\n",
    "            print(\"prepare label: '{}'\".format(label_name))\n",
    "            \n",
    "            # prepare subdir for label images\n",
    "            label_subdir = os.path.join(self.dataset_name, label_name)\n",
    "            self.create_dir_if_not_exists(label_subdir)\n",
    "\n",
    "            # find images\n",
    "            image_urls = self.get_image_urls(search_keyword, search_limit_per_class)\n",
    "            print(\"found {} images for '{}'\".format(len(image_urls), search_keyword))\n",
    "\n",
    "            # save images\n",
    "            # print results every 10%\n",
    "            logging_step = int(0.1 * len(image_urls))\n",
    "            logging_step = 1 if logging_step == 0 else logging_step\n",
    "            \n",
    "            for n, image_url in enumerate(image_urls):\n",
    "                extension_pos = image_url.rfind('.')\n",
    "                if -1 == extension_pos:\n",
    "                    continue\n",
    "                image_path = os.path.join(label_subdir, str(n) + image_url[extension_pos:])\n",
    "                \n",
    "                image_bytes = self.download_url_image_bytes(image_url)\n",
    "                if image_bytes is None:\n",
    "                    #print('no image bytes')\n",
    "                    continue\n",
    "\n",
    "                self.save_image_bytes(image_bytes, image_path)\n",
    "\n",
    "                if n % logging_step == 0:\n",
    "                    percent_done = n / len(image_urls) * 100.\n",
    "                    print(\"processed {} for {:.1f}%\".format(label_name, percent_done))\n",
    "                    \n",
    "        print(\"dataset downloading done.\")\n",
    "                \n",
    "                \n",
    "    def create_dir_if_not_exists(self, dir_name):\n",
    "        if not os.path.isdir(dir_name):\n",
    "            print('create dir:', dir_name)\n",
    "            os.mkdir(dir_name)\n",
    "\n",
    "    def get_image_urls(self, keywords, maxnum):\n",
    "        search_result_data = ddg_images(keywords, region='wt-wt', safesearch='Moderate',\n",
    "                        size='Medium',# license_image='Public',\n",
    "                        max_results=maxnum, download=False)\n",
    "\n",
    "        image_urls = [\n",
    "            data['image'] for data in search_result_data\n",
    "            # check file extension\n",
    "            if any(data['image'].endswith(ext) for ext in self.allowed_extensions)\n",
    "        ]\n",
    "        return image_urls\n",
    "        \n",
    "    def download_url_image_bytes(self, image_url):\n",
    "        try:\n",
    "            response = requests.get(image_url, timeout=(1.5, 5.5))# 1.5 sec - connect, 5.5 sec - read\n",
    "        except (requests.exceptions.ConnectTimeout,\n",
    "                requests.exceptions.ReadTimeout,\n",
    "                requests.exceptions.ConnectionError):\n",
    "            print('timeout for URL:', image_url)\n",
    "            return\n",
    "            \n",
    "        if not response.ok:\n",
    "            print('file cannot be downloaded:', image_url)\n",
    "            return\n",
    "\n",
    "        # get content\n",
    "        image_data = response.content\n",
    "\n",
    "        return image_data\n",
    "    \n",
    "    def save_image_bytes(self, image_bytes, image_savepath):\n",
    "        with open(image_savepath, 'wb') as image_file:\n",
    "            image_file.write(image_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1ec37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_downloader = DatasetDownloader(dataset_name='bears_dataset',\n",
    "    label_data=[\n",
    "        ('black', 'black bear'),\n",
    "        ('grizzly', 'grizzly bear'),\n",
    "        ('teddy', 'teddy bear')],\n",
    "    allowed_extensions=['.jpg', '.png', '.jpeg'])\n",
    "dataset_downloader.download_images(search_limit_per_class=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d955b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
